{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "noAOCXnjE72s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b70db6a-2aca-401f-deaf-dc0d8cf4979c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "8FFWNvYGFbzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/keypoints_final_256.csv')\n",
        "df_test = pd.read_csv('/content/drive/MyDrive/keypoints_final_test_256.csv')"
      ],
      "metadata": {
        "id": "dlG98oieFm4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop('Unnamed: 0',1)\n",
        "df_test = df_test.drop('Unnamed: 0',1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEI3i23WFtV6",
        "outputId": "161dabc5-b022-43f0-c65b-18d925183789"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-79-2408e182c2cf>:1: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  df = df.drop('Unnamed: 0',1)\n",
            "<ipython-input-79-2408e182c2cf>:2: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  df_test = df_test.drop('Unnamed: 0',1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "1SDKesvSFuEe",
        "outputId": "735bcd19-1a9a-4968-cbbd-644e92dfbda7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              0         1         2         3         4         5         6  \\\n",
              "0      0.561012  0.302157 -0.123826  0.564638  0.289460 -0.105465  0.567299   \n",
              "1      0.561800  0.300378 -0.112502  0.564178  0.286860 -0.094147  0.566694   \n",
              "2      0.561128  0.303233 -0.398081  0.564855  0.286085 -0.388905  0.567303   \n",
              "3      0.560880  0.309663 -0.300727  0.564582  0.291333 -0.289611  0.567254   \n",
              "4      0.561824  0.313649 -0.342597  0.565599  0.294846 -0.333475  0.568605   \n",
              "...         ...       ...       ...       ...       ...       ...       ...   \n",
              "13124  0.486648  0.286260 -0.089110  0.490879  0.273558 -0.067470  0.493832   \n",
              "13125  0.482443  0.300422 -0.362747  0.487832  0.286010 -0.350695  0.491121   \n",
              "13126  0.480868  0.294958 -0.321305  0.485442  0.280226 -0.312262  0.488200   \n",
              "13127  0.476346  0.293068 -0.347669  0.480525  0.276444 -0.338560  0.483666   \n",
              "13128  0.474513  0.301189 -0.396490  0.478878  0.286351 -0.384223  0.482274   \n",
              "\n",
              "              7         8         9  ...        90        91        92  \\\n",
              "0      0.289761 -0.105632  0.569772  ...  0.533098  0.883884  0.116445   \n",
              "1      0.287539 -0.094309  0.568935  ...  0.532270  0.883887  0.118631   \n",
              "2      0.285346 -0.389124  0.569531  ...  0.527861  0.878289  0.133518   \n",
              "3      0.290768 -0.289883  0.570002  ...  0.536183  0.912010  0.188811   \n",
              "4      0.294382 -0.333750  0.571083  ...  0.540910  0.887583  0.192771   \n",
              "...         ...       ...       ...  ...       ...       ...       ...   \n",
              "13124  0.272559 -0.067595  0.497064  ...  0.470475  0.872472  0.102894   \n",
              "13125  0.285521 -0.350950  0.493022  ...  0.474411  0.877378  0.126974   \n",
              "13126  0.280182 -0.312505  0.491096  ...  0.473303  0.876180  0.188003   \n",
              "13127  0.276016 -0.338818  0.485902  ...  0.473899  0.877159  0.200288   \n",
              "13128  0.286848 -0.384487  0.484189  ...  0.473569  0.875810  0.184950   \n",
              "\n",
              "             93        94        95        96        97        98       1.1  \n",
              "0      0.566635  0.933335  0.036987  0.536138  0.938561 -0.000178  tadasana  \n",
              "1      0.570173  0.931963  0.035905  0.532538  0.933740 -0.001898  tadasana  \n",
              "2      0.566152  0.918341  0.058699  0.534654  0.928207 -0.001211  tadasana  \n",
              "3      0.567818  0.914286  0.152388  0.530323  0.931029  0.077729  tadasana  \n",
              "4      0.571397  0.923574  0.116451  0.531012  0.930395  0.070042  tadasana  \n",
              "...         ...       ...       ...       ...       ...       ...       ...  \n",
              "13124  0.496999  0.912968  0.022522  0.466042  0.915865 -0.025367  tadasana  \n",
              "13125  0.494784  0.912487  0.072589  0.465412  0.914555  0.006590  tadasana  \n",
              "13126  0.495985  0.914854  0.099370  0.465068  0.916696  0.075990  tadasana  \n",
              "13127  0.494451  0.915021  0.085202  0.464427  0.913910  0.084059  tadasana  \n",
              "13128  0.496280  0.913268  0.085682  0.464682  0.914772  0.071310  tadasana  \n",
              "\n",
              "[13129 rows x 100 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b5b218d4-2eda-4667-bb0a-fd55577613a6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>1.1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.561012</td>\n",
              "      <td>0.302157</td>\n",
              "      <td>-0.123826</td>\n",
              "      <td>0.564638</td>\n",
              "      <td>0.289460</td>\n",
              "      <td>-0.105465</td>\n",
              "      <td>0.567299</td>\n",
              "      <td>0.289761</td>\n",
              "      <td>-0.105632</td>\n",
              "      <td>0.569772</td>\n",
              "      <td>...</td>\n",
              "      <td>0.533098</td>\n",
              "      <td>0.883884</td>\n",
              "      <td>0.116445</td>\n",
              "      <td>0.566635</td>\n",
              "      <td>0.933335</td>\n",
              "      <td>0.036987</td>\n",
              "      <td>0.536138</td>\n",
              "      <td>0.938561</td>\n",
              "      <td>-0.000178</td>\n",
              "      <td>tadasana</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.561800</td>\n",
              "      <td>0.300378</td>\n",
              "      <td>-0.112502</td>\n",
              "      <td>0.564178</td>\n",
              "      <td>0.286860</td>\n",
              "      <td>-0.094147</td>\n",
              "      <td>0.566694</td>\n",
              "      <td>0.287539</td>\n",
              "      <td>-0.094309</td>\n",
              "      <td>0.568935</td>\n",
              "      <td>...</td>\n",
              "      <td>0.532270</td>\n",
              "      <td>0.883887</td>\n",
              "      <td>0.118631</td>\n",
              "      <td>0.570173</td>\n",
              "      <td>0.931963</td>\n",
              "      <td>0.035905</td>\n",
              "      <td>0.532538</td>\n",
              "      <td>0.933740</td>\n",
              "      <td>-0.001898</td>\n",
              "      <td>tadasana</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.561128</td>\n",
              "      <td>0.303233</td>\n",
              "      <td>-0.398081</td>\n",
              "      <td>0.564855</td>\n",
              "      <td>0.286085</td>\n",
              "      <td>-0.388905</td>\n",
              "      <td>0.567303</td>\n",
              "      <td>0.285346</td>\n",
              "      <td>-0.389124</td>\n",
              "      <td>0.569531</td>\n",
              "      <td>...</td>\n",
              "      <td>0.527861</td>\n",
              "      <td>0.878289</td>\n",
              "      <td>0.133518</td>\n",
              "      <td>0.566152</td>\n",
              "      <td>0.918341</td>\n",
              "      <td>0.058699</td>\n",
              "      <td>0.534654</td>\n",
              "      <td>0.928207</td>\n",
              "      <td>-0.001211</td>\n",
              "      <td>tadasana</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.560880</td>\n",
              "      <td>0.309663</td>\n",
              "      <td>-0.300727</td>\n",
              "      <td>0.564582</td>\n",
              "      <td>0.291333</td>\n",
              "      <td>-0.289611</td>\n",
              "      <td>0.567254</td>\n",
              "      <td>0.290768</td>\n",
              "      <td>-0.289883</td>\n",
              "      <td>0.570002</td>\n",
              "      <td>...</td>\n",
              "      <td>0.536183</td>\n",
              "      <td>0.912010</td>\n",
              "      <td>0.188811</td>\n",
              "      <td>0.567818</td>\n",
              "      <td>0.914286</td>\n",
              "      <td>0.152388</td>\n",
              "      <td>0.530323</td>\n",
              "      <td>0.931029</td>\n",
              "      <td>0.077729</td>\n",
              "      <td>tadasana</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.561824</td>\n",
              "      <td>0.313649</td>\n",
              "      <td>-0.342597</td>\n",
              "      <td>0.565599</td>\n",
              "      <td>0.294846</td>\n",
              "      <td>-0.333475</td>\n",
              "      <td>0.568605</td>\n",
              "      <td>0.294382</td>\n",
              "      <td>-0.333750</td>\n",
              "      <td>0.571083</td>\n",
              "      <td>...</td>\n",
              "      <td>0.540910</td>\n",
              "      <td>0.887583</td>\n",
              "      <td>0.192771</td>\n",
              "      <td>0.571397</td>\n",
              "      <td>0.923574</td>\n",
              "      <td>0.116451</td>\n",
              "      <td>0.531012</td>\n",
              "      <td>0.930395</td>\n",
              "      <td>0.070042</td>\n",
              "      <td>tadasana</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13124</th>\n",
              "      <td>0.486648</td>\n",
              "      <td>0.286260</td>\n",
              "      <td>-0.089110</td>\n",
              "      <td>0.490879</td>\n",
              "      <td>0.273558</td>\n",
              "      <td>-0.067470</td>\n",
              "      <td>0.493832</td>\n",
              "      <td>0.272559</td>\n",
              "      <td>-0.067595</td>\n",
              "      <td>0.497064</td>\n",
              "      <td>...</td>\n",
              "      <td>0.470475</td>\n",
              "      <td>0.872472</td>\n",
              "      <td>0.102894</td>\n",
              "      <td>0.496999</td>\n",
              "      <td>0.912968</td>\n",
              "      <td>0.022522</td>\n",
              "      <td>0.466042</td>\n",
              "      <td>0.915865</td>\n",
              "      <td>-0.025367</td>\n",
              "      <td>tadasana</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13125</th>\n",
              "      <td>0.482443</td>\n",
              "      <td>0.300422</td>\n",
              "      <td>-0.362747</td>\n",
              "      <td>0.487832</td>\n",
              "      <td>0.286010</td>\n",
              "      <td>-0.350695</td>\n",
              "      <td>0.491121</td>\n",
              "      <td>0.285521</td>\n",
              "      <td>-0.350950</td>\n",
              "      <td>0.493022</td>\n",
              "      <td>...</td>\n",
              "      <td>0.474411</td>\n",
              "      <td>0.877378</td>\n",
              "      <td>0.126974</td>\n",
              "      <td>0.494784</td>\n",
              "      <td>0.912487</td>\n",
              "      <td>0.072589</td>\n",
              "      <td>0.465412</td>\n",
              "      <td>0.914555</td>\n",
              "      <td>0.006590</td>\n",
              "      <td>tadasana</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13126</th>\n",
              "      <td>0.480868</td>\n",
              "      <td>0.294958</td>\n",
              "      <td>-0.321305</td>\n",
              "      <td>0.485442</td>\n",
              "      <td>0.280226</td>\n",
              "      <td>-0.312262</td>\n",
              "      <td>0.488200</td>\n",
              "      <td>0.280182</td>\n",
              "      <td>-0.312505</td>\n",
              "      <td>0.491096</td>\n",
              "      <td>...</td>\n",
              "      <td>0.473303</td>\n",
              "      <td>0.876180</td>\n",
              "      <td>0.188003</td>\n",
              "      <td>0.495985</td>\n",
              "      <td>0.914854</td>\n",
              "      <td>0.099370</td>\n",
              "      <td>0.465068</td>\n",
              "      <td>0.916696</td>\n",
              "      <td>0.075990</td>\n",
              "      <td>tadasana</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13127</th>\n",
              "      <td>0.476346</td>\n",
              "      <td>0.293068</td>\n",
              "      <td>-0.347669</td>\n",
              "      <td>0.480525</td>\n",
              "      <td>0.276444</td>\n",
              "      <td>-0.338560</td>\n",
              "      <td>0.483666</td>\n",
              "      <td>0.276016</td>\n",
              "      <td>-0.338818</td>\n",
              "      <td>0.485902</td>\n",
              "      <td>...</td>\n",
              "      <td>0.473899</td>\n",
              "      <td>0.877159</td>\n",
              "      <td>0.200288</td>\n",
              "      <td>0.494451</td>\n",
              "      <td>0.915021</td>\n",
              "      <td>0.085202</td>\n",
              "      <td>0.464427</td>\n",
              "      <td>0.913910</td>\n",
              "      <td>0.084059</td>\n",
              "      <td>tadasana</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13128</th>\n",
              "      <td>0.474513</td>\n",
              "      <td>0.301189</td>\n",
              "      <td>-0.396490</td>\n",
              "      <td>0.478878</td>\n",
              "      <td>0.286351</td>\n",
              "      <td>-0.384223</td>\n",
              "      <td>0.482274</td>\n",
              "      <td>0.286848</td>\n",
              "      <td>-0.384487</td>\n",
              "      <td>0.484189</td>\n",
              "      <td>...</td>\n",
              "      <td>0.473569</td>\n",
              "      <td>0.875810</td>\n",
              "      <td>0.184950</td>\n",
              "      <td>0.496280</td>\n",
              "      <td>0.913268</td>\n",
              "      <td>0.085682</td>\n",
              "      <td>0.464682</td>\n",
              "      <td>0.914772</td>\n",
              "      <td>0.071310</td>\n",
              "      <td>tadasana</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>13129 rows × 100 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b5b218d4-2eda-4667-bb0a-fd55577613a6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b5b218d4-2eda-4667-bb0a-fd55577613a6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b5b218d4-2eda-4667-bb0a-fd55577613a6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "TsIIleLCF0kQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lbe = LabelEncoder()\n",
        "df['1.1']= lbe.fit_transform(df['1.1'])\n",
        "df_test['1.1']= lbe.fit_transform(df_test['1.1'])\n",
        "dict(zip(lbe.classes_, lbe.transform(lbe.classes_)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mheSh8XMrXRy",
        "outputId": "e574907e-afde-4cf3-ac2b-9cc33e6f4147"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bhujan': 0, 'padmasan': 1, 'shav': 2, 'tadasana': 3, 'trik': 4, 'vriksh': 5}"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "P8HAO9N7sX4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class YogaDataset(Dataset):\n",
        "  def __init__(self, df):\n",
        "    self.features = df.drop(['1.1'], axis=1)\n",
        "    self.target = df['1.1']\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.features)\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    features = self.features.loc[index]\n",
        "    target = self.target[index]\n",
        "    return torch.tensor(features.tolist()).float().to(device), torch.tensor(target).long().to(device)"
      ],
      "metadata": {
        "id": "04UjZs3SGFt9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_feat = df.drop(['1.1'], axis=1)"
      ],
      "metadata": {
        "id": "kjn1diRCPPBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "oLFPQdOlPo4_",
        "outputId": "40747ab4-fe4b-472c-c0f0-a1948041777e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# target = df['1.1']"
      ],
      "metadata": {
        "id": "6rS3CyegPVHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = YogaDataset(df)"
      ],
      "metadata": {
        "id": "AUf74C14Pc3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feat, target = data[100]"
      ],
      "metadata": {
        "id": "LIEICAjrPi7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZD7k54HhXok",
        "outputId": "4b962af8-b643-4f34-b9c6-dcedb20bfa79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3)"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN(nn.Module):\n",
        "  def __init__(self,hidden_dim=20, input_dim=99, sequence_num=16, n_layers=1):\n",
        "    super(RNN, self).__init__()\n",
        "    self.input_dim = input_dim\n",
        "    self.sequence_num = sequence_num\n",
        "    self.n_layers = n_layers\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.rnn = nn.RNN(input_dim,hidden_dim, n_layers, batch_first=True)\n",
        "    self.fc = nn.Linear(hidden_dim, 6)\n",
        "  def forward(self, input):\n",
        "    h0 = torch.zeros(self.n_layers,  self.hidden_dim)\n",
        "    out, _ = self.rnn(input,h0)\n",
        "    pred = self.fc(out)\n",
        "    output = nn.Softmax( dim=1)(pred)\n",
        "    return output"
      ],
      "metadata": {
        "id": "NHqLquJOhgNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = RNN().to(device)"
      ],
      "metadata": {
        "id": "1RWWhgIds98B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "JH5Wcyq4tAd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "LUqs5IautI7r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a63a6d9-c565-4497-971a-5c7da46031c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNN(\n",
              "  (rnn): RNN(99, 20, batch_first=True)\n",
              "  (fc): Linear(in_features=20, out_features=6, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xzg1h1i1g9KR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data():\n",
        "    train = YogaDataset(df)\n",
        "    test = YogaDataset(df_test)\n",
        "    trn_dl = DataLoader(train, batch_size=16,shuffle=True, drop_last=True)\n",
        "    test_dl = DataLoader(test, batch_size=16, shuffle=True, drop_last=True)\n",
        "    return trn_dl, test_dl"
      ],
      "metadata": {
        "id": "lslJszPoXFq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trn_ldr, test_ldr = get_data()\n",
        "# for x in trn_ldr:\n",
        "#   print(x)\n",
        "#   break"
      ],
      "metadata": {
        "id": "EMNUp4xCXnuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_batch(x, y, model, loss_fn, opt):\n",
        "    model.train()\n",
        "    prediction = model(x)\n",
        "    # print(y.shape)\n",
        "    # print(y)\n",
        "    # print(prediction.shape)\n",
        "    batch_loss = loss_fn(prediction, y)\n",
        "    batch_loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    return batch_loss.item()"
      ],
      "metadata": {
        "id": "Wf3uvwm2Xwjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def accuracy(x, y, model):\n",
        "    model.eval()\n",
        "    pred = model(x)\n",
        "    _, is_correct = torch.max(pred.data,1)\n",
        "    acc_all = (is_correct==y).detach().cpu().numpy()\n",
        "    return acc_all"
      ],
      "metadata": {
        "id": "PEY2jjXIX3nZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def val_loss_trn(x, y, model, loss_fn):\n",
        "  prediction = model(x)\n",
        "  val_loss = loss_fn(prediction, y)\n",
        "  return val_loss.item()"
      ],
      "metadata": {
        "id": "k4TGtcVqjmIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss = []\n",
        "train_accuracies= []\n",
        "val_loss = [] \n",
        "val_accuracies = []\n",
        "\n",
        "for i in range(10):\n",
        "    print(f'Epoch: _________*****{i}*****_______')\n",
        "    train_epoch_losses, train_epoch_accuracies = [], []\n",
        "    val_epoch_accuracies, val_epoch_losses = [], []\n",
        "\n",
        "\n",
        "\n",
        "    for ix, batch in (enumerate(iter(trn_ldr))):\n",
        "\n",
        "        x, y = batch\n",
        "        train_epoch_losses.append(train_batch(x, y, model, loss_fn, optimizer))\n",
        "    train_epoch_loss = np.array(train_epoch_losses).mean()\n",
        "    print(f'Epoch: _________*****{i} Training Loss : {train_epoch_loss} *****_______')\n",
        "\n",
        "\n",
        "\n",
        "    for ix, batch in (enumerate(iter(trn_ldr))):\n",
        "        x, y = batch\n",
        "        # x = x.permute(0, 3,1,2)\n",
        "        train_epoch_accuracies.append(sum(accuracy(x, y, model)) / len(y))\n",
        "    train_epoch_accuracy = np.array(train_epoch_accuracies).mean()\n",
        "    print(f'Epoch: _________*****{i} Training Accuracy: {train_epoch_accuracy} *****_______')\n",
        "\n",
        "\n",
        "    for ix, batch in (enumerate(iter(test_ldr))):\n",
        "        x, y = batch\n",
        "        # x = x.permute(0, 3,1,2)\n",
        "        val_epoch_losses.append(val_loss_trn(x, y, model, loss_fn))\n",
        "    val_epoch_loss = np.array(val_epoch_losses).mean()\n",
        "    print(f'Epoch: _________*****{i} Validation Loss : {val_epoch_loss} *****_______')\n",
        "\n",
        "    for ix, batch in (enumerate(iter(test_ldr))):\n",
        "        x, y = batch\n",
        "        # x = x.permute(0, 3,1,2)\n",
        "        val_epoch_accuracies.append(sum(accuracy(x, y, model)) / len(y))\n",
        "    val_epoch_accuracy = np.array(val_epoch_accuracies).mean()\n",
        "    print(f'Epoch: _________*****{i} Validation Accuracy. {val_epoch_accuracy} *****_______')\n",
        "    print('\\n')\n",
        "\n",
        "    train_loss.append(train_epoch_loss)\n",
        "    train_accuracies.append(train_epoch_accuracy)\n",
        "    val_accuracies.append(val_epoch_accuracy)\n",
        "    val_loss.append(val_epoch_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLoo8XM3X6Pn",
        "outputId": "3dbc397d-9651-415f-dc93-48278f1f6ba6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: _________*****0*****_______\n",
            "Epoch: _________*****0 Training Loss : 1.3545948684215545 *****_______\n",
            "Epoch: _________*****0 Training Accuracy: 0.9480182926829268 *****_______\n",
            "Epoch: _________*****0 Validation Loss : 1.1210019684004036 *****_______\n",
            "Epoch: _________*****0 Validation Accuracy. 0.9666811846689896 *****_______\n",
            "\n",
            "\n",
            "Epoch: _________*****1*****_______\n",
            "Epoch: _________*****1 Training Loss : 1.116492057864259 *****_______\n",
            "Epoch: _________*****1 Training Accuracy: 0.9556402439024391 *****_______\n",
            "Epoch: _________*****1 Validation Loss : 1.0802388963799028 *****_______\n",
            "Epoch: _________*****1 Validation Accuracy. 0.98301393728223 *****_______\n",
            "\n",
            "\n",
            "Epoch: _________*****2*****_______\n",
            "Epoch: _________*****2 Training Loss : 1.0952995152008242 *****_______\n",
            "Epoch: _________*****2 Training Accuracy: 0.9614329268292683 *****_______\n",
            "Epoch: _________*****2 Validation Loss : 1.0852552743738952 *****_______\n",
            "Epoch: _________*****2 Validation Accuracy. 0.9671167247386759 *****_______\n",
            "\n",
            "\n",
            "Epoch: _________*****3*****_______\n",
            "Epoch: _________*****3 Training Loss : 1.0847660637483365 *****_______\n",
            "Epoch: _________*****3 Training Accuracy: 0.9791158536585366 *****_______\n",
            "Epoch: _________*****3 Validation Loss : 1.0828074387141637 *****_______\n",
            "Epoch: _________*****3 Validation Accuracy. 0.9760452961672473 *****_______\n",
            "\n",
            "\n",
            "Epoch: _________*****4*****_______\n",
            "Epoch: _________*****4 Training Loss : 1.0721324951183506 *****_______\n",
            "Epoch: _________*****4 Training Accuracy: 0.9820884146341463 *****_______\n",
            "Epoch: _________*****4 Validation Loss : 1.062323013664538 *****_______\n",
            "Epoch: _________*****4 Validation Accuracy. 0.9880226480836237 *****_______\n",
            "\n",
            "\n",
            "Epoch: _________*****5*****_______\n",
            "Epoch: _________*****5 Training Loss : 1.0665623484588251 *****_______\n",
            "Epoch: _________*****5 Training Accuracy: 0.9841463414634146 *****_______\n",
            "Epoch: _________*****5 Validation Loss : 1.0621302389517062 *****_______\n",
            "Epoch: _________*****5 Validation Accuracy. 0.9878048780487805 *****_______\n",
            "\n",
            "\n",
            "Epoch: _________*****6*****_______\n",
            "Epoch: _________*****6 Training Loss : 1.0633735297656641 *****_______\n",
            "Epoch: _________*****6 Training Accuracy: 0.9867378048780487 *****_______\n",
            "Epoch: _________*****6 Validation Loss : 1.0588546065087934 *****_______\n",
            "Epoch: _________*****6 Validation Accuracy. 0.9884581881533101 *****_______\n",
            "\n",
            "\n",
            "Epoch: _________*****7*****_______\n",
            "Epoch: _________*****7 Training Loss : 1.0614071011543273 *****_______\n",
            "Epoch: _________*****7 Training Accuracy: 0.9857469512195122 *****_______\n",
            "Epoch: _________*****7 Validation Loss : 1.0623281994763152 *****_______\n",
            "Epoch: _________*****7 Validation Accuracy. 0.9862804878048781 *****_______\n",
            "\n",
            "\n",
            "Epoch: _________*****8*****_______\n",
            "Epoch: _________*****8 Training Loss : 1.060263218966926 *****_______\n",
            "Epoch: _________*****8 Training Accuracy: 0.9874237804878049 *****_______\n",
            "Epoch: _________*****8 Validation Loss : 1.057521865758331 *****_______\n",
            "Epoch: _________*****8 Validation Accuracy. 0.9871515679442509 *****_______\n",
            "\n",
            "\n",
            "Epoch: _________*****9*****_______\n",
            "Epoch: _________*****9 Training Loss : 1.0589083960870418 *****_______\n",
            "Epoch: _________*****9 Training Accuracy: 0.9874237804878049 *****_______\n",
            "Epoch: _________*****9 Validation Loss : 1.0569080068674652 *****_______\n",
            "Epoch: _________*****9 Validation Accuracy. 0.9882404181184669 *****_______\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_val = pd.read_csv('/content/drive/MyDrive/keypoints_final_val_256.csv').drop('Unnamed: 0',1)"
      ],
      "metadata": {
        "id": "fE2v2W1UYH2_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e89ac808-7093-4718-ecc1-8d5da33adaf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-106-a67a6a83625a>:1: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  df_val = pd.read_csv('/content/drive/MyDrive/keypoints_final_val_256.csv').drop('Unnamed: 0',1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_val[512:(512+256)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "bt1ZRN3AConI",
        "outputId": "7d0e5a4c-08f0-4838-ac5b-3e853f258e5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            0         1         2         3         4         5         6  \\\n",
              "512  0.498204  0.325045 -0.280092  0.501241  0.313319 -0.268156  0.504362   \n",
              "513  0.497350  0.325974 -0.288618  0.500722  0.314890 -0.276627  0.503065   \n",
              "514  0.495267  0.325460 -0.297356  0.498773  0.313994 -0.284876  0.501191   \n",
              "515  0.491493  0.325750 -0.294950  0.494320  0.313288 -0.282730  0.497406   \n",
              "516  0.489973  0.323742 -0.254229  0.493444  0.311668 -0.241386  0.495739   \n",
              "..        ...       ...       ...       ...       ...       ...       ...   \n",
              "763  0.467412  0.332124 -0.161435  0.471836  0.319425 -0.143835  0.474202   \n",
              "764  0.468211  0.333263 -0.166658  0.472588  0.320537 -0.149037  0.474694   \n",
              "765  0.469188  0.333790 -0.164722  0.473254  0.320903 -0.147057  0.475751   \n",
              "766  0.469656  0.332489 -0.155165  0.473491  0.319844 -0.137579  0.476636   \n",
              "767  0.470041  0.331704 -0.145384  0.473747  0.319283 -0.127014  0.476717   \n",
              "\n",
              "            7         8         9  ...        90        91        92  \\\n",
              "512  0.314049 -0.268400  0.506766  ...  0.471870  0.837673  0.210182   \n",
              "513  0.314921 -0.276867  0.505141  ...  0.472278  0.835503  0.208577   \n",
              "514  0.314042 -0.285120  0.503176  ...  0.471952  0.835442  0.214445   \n",
              "515  0.313282 -0.282968  0.499448  ...  0.472206  0.835779  0.202842   \n",
              "516  0.311929 -0.241621  0.498364  ...  0.472216  0.835538  0.191708   \n",
              "..        ...       ...       ...  ...       ...       ...       ...   \n",
              "763  0.319999 -0.143992  0.476708  ...  0.475597  0.831335  0.103849   \n",
              "764  0.320821 -0.149193  0.477804  ...  0.476207  0.828265  0.104060   \n",
              "765  0.320856 -0.147215  0.479065  ...  0.475647  0.828216  0.094975   \n",
              "766  0.319893 -0.137735  0.479524  ...  0.476204  0.829023  0.107831   \n",
              "767  0.319353 -0.127165  0.479569  ...  0.476365  0.828813  0.106276   \n",
              "\n",
              "           93        94        95        96        97        98     1.1  \n",
              "512  0.500933  0.863186  0.104345  0.443333  0.851790  0.119382  vriksh  \n",
              "513  0.500395  0.863170  0.087212  0.443110  0.854528  0.116487  vriksh  \n",
              "514  0.499915  0.863171  0.099858  0.443945  0.853825  0.122538  vriksh  \n",
              "515  0.499877  0.863686  0.083113  0.443558  0.854689  0.108751  vriksh  \n",
              "516  0.498591  0.861988  0.105938  0.445896  0.852941  0.097599  vriksh  \n",
              "..        ...       ...       ...       ...       ...       ...     ...  \n",
              "763  0.472315  0.709288  0.044309  0.463503  0.858247  0.023949  vriksh  \n",
              "764  0.471883  0.710419  0.036686  0.464533  0.858780  0.024493  vriksh  \n",
              "765  0.471934  0.709247  0.044217  0.465466  0.859077  0.015660  vriksh  \n",
              "766  0.472267  0.711030  0.044306  0.465084  0.859327  0.029730  vriksh  \n",
              "767  0.471778  0.711524  0.044861  0.464475  0.860323  0.030196  vriksh  \n",
              "\n",
              "[256 rows x 100 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-799f8e53-eda2-40a9-af16-a900788bbd4a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>1.1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>512</th>\n",
              "      <td>0.498204</td>\n",
              "      <td>0.325045</td>\n",
              "      <td>-0.280092</td>\n",
              "      <td>0.501241</td>\n",
              "      <td>0.313319</td>\n",
              "      <td>-0.268156</td>\n",
              "      <td>0.504362</td>\n",
              "      <td>0.314049</td>\n",
              "      <td>-0.268400</td>\n",
              "      <td>0.506766</td>\n",
              "      <td>...</td>\n",
              "      <td>0.471870</td>\n",
              "      <td>0.837673</td>\n",
              "      <td>0.210182</td>\n",
              "      <td>0.500933</td>\n",
              "      <td>0.863186</td>\n",
              "      <td>0.104345</td>\n",
              "      <td>0.443333</td>\n",
              "      <td>0.851790</td>\n",
              "      <td>0.119382</td>\n",
              "      <td>vriksh</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>513</th>\n",
              "      <td>0.497350</td>\n",
              "      <td>0.325974</td>\n",
              "      <td>-0.288618</td>\n",
              "      <td>0.500722</td>\n",
              "      <td>0.314890</td>\n",
              "      <td>-0.276627</td>\n",
              "      <td>0.503065</td>\n",
              "      <td>0.314921</td>\n",
              "      <td>-0.276867</td>\n",
              "      <td>0.505141</td>\n",
              "      <td>...</td>\n",
              "      <td>0.472278</td>\n",
              "      <td>0.835503</td>\n",
              "      <td>0.208577</td>\n",
              "      <td>0.500395</td>\n",
              "      <td>0.863170</td>\n",
              "      <td>0.087212</td>\n",
              "      <td>0.443110</td>\n",
              "      <td>0.854528</td>\n",
              "      <td>0.116487</td>\n",
              "      <td>vriksh</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>514</th>\n",
              "      <td>0.495267</td>\n",
              "      <td>0.325460</td>\n",
              "      <td>-0.297356</td>\n",
              "      <td>0.498773</td>\n",
              "      <td>0.313994</td>\n",
              "      <td>-0.284876</td>\n",
              "      <td>0.501191</td>\n",
              "      <td>0.314042</td>\n",
              "      <td>-0.285120</td>\n",
              "      <td>0.503176</td>\n",
              "      <td>...</td>\n",
              "      <td>0.471952</td>\n",
              "      <td>0.835442</td>\n",
              "      <td>0.214445</td>\n",
              "      <td>0.499915</td>\n",
              "      <td>0.863171</td>\n",
              "      <td>0.099858</td>\n",
              "      <td>0.443945</td>\n",
              "      <td>0.853825</td>\n",
              "      <td>0.122538</td>\n",
              "      <td>vriksh</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>515</th>\n",
              "      <td>0.491493</td>\n",
              "      <td>0.325750</td>\n",
              "      <td>-0.294950</td>\n",
              "      <td>0.494320</td>\n",
              "      <td>0.313288</td>\n",
              "      <td>-0.282730</td>\n",
              "      <td>0.497406</td>\n",
              "      <td>0.313282</td>\n",
              "      <td>-0.282968</td>\n",
              "      <td>0.499448</td>\n",
              "      <td>...</td>\n",
              "      <td>0.472206</td>\n",
              "      <td>0.835779</td>\n",
              "      <td>0.202842</td>\n",
              "      <td>0.499877</td>\n",
              "      <td>0.863686</td>\n",
              "      <td>0.083113</td>\n",
              "      <td>0.443558</td>\n",
              "      <td>0.854689</td>\n",
              "      <td>0.108751</td>\n",
              "      <td>vriksh</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>516</th>\n",
              "      <td>0.489973</td>\n",
              "      <td>0.323742</td>\n",
              "      <td>-0.254229</td>\n",
              "      <td>0.493444</td>\n",
              "      <td>0.311668</td>\n",
              "      <td>-0.241386</td>\n",
              "      <td>0.495739</td>\n",
              "      <td>0.311929</td>\n",
              "      <td>-0.241621</td>\n",
              "      <td>0.498364</td>\n",
              "      <td>...</td>\n",
              "      <td>0.472216</td>\n",
              "      <td>0.835538</td>\n",
              "      <td>0.191708</td>\n",
              "      <td>0.498591</td>\n",
              "      <td>0.861988</td>\n",
              "      <td>0.105938</td>\n",
              "      <td>0.445896</td>\n",
              "      <td>0.852941</td>\n",
              "      <td>0.097599</td>\n",
              "      <td>vriksh</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>763</th>\n",
              "      <td>0.467412</td>\n",
              "      <td>0.332124</td>\n",
              "      <td>-0.161435</td>\n",
              "      <td>0.471836</td>\n",
              "      <td>0.319425</td>\n",
              "      <td>-0.143835</td>\n",
              "      <td>0.474202</td>\n",
              "      <td>0.319999</td>\n",
              "      <td>-0.143992</td>\n",
              "      <td>0.476708</td>\n",
              "      <td>...</td>\n",
              "      <td>0.475597</td>\n",
              "      <td>0.831335</td>\n",
              "      <td>0.103849</td>\n",
              "      <td>0.472315</td>\n",
              "      <td>0.709288</td>\n",
              "      <td>0.044309</td>\n",
              "      <td>0.463503</td>\n",
              "      <td>0.858247</td>\n",
              "      <td>0.023949</td>\n",
              "      <td>vriksh</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>764</th>\n",
              "      <td>0.468211</td>\n",
              "      <td>0.333263</td>\n",
              "      <td>-0.166658</td>\n",
              "      <td>0.472588</td>\n",
              "      <td>0.320537</td>\n",
              "      <td>-0.149037</td>\n",
              "      <td>0.474694</td>\n",
              "      <td>0.320821</td>\n",
              "      <td>-0.149193</td>\n",
              "      <td>0.477804</td>\n",
              "      <td>...</td>\n",
              "      <td>0.476207</td>\n",
              "      <td>0.828265</td>\n",
              "      <td>0.104060</td>\n",
              "      <td>0.471883</td>\n",
              "      <td>0.710419</td>\n",
              "      <td>0.036686</td>\n",
              "      <td>0.464533</td>\n",
              "      <td>0.858780</td>\n",
              "      <td>0.024493</td>\n",
              "      <td>vriksh</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>765</th>\n",
              "      <td>0.469188</td>\n",
              "      <td>0.333790</td>\n",
              "      <td>-0.164722</td>\n",
              "      <td>0.473254</td>\n",
              "      <td>0.320903</td>\n",
              "      <td>-0.147057</td>\n",
              "      <td>0.475751</td>\n",
              "      <td>0.320856</td>\n",
              "      <td>-0.147215</td>\n",
              "      <td>0.479065</td>\n",
              "      <td>...</td>\n",
              "      <td>0.475647</td>\n",
              "      <td>0.828216</td>\n",
              "      <td>0.094975</td>\n",
              "      <td>0.471934</td>\n",
              "      <td>0.709247</td>\n",
              "      <td>0.044217</td>\n",
              "      <td>0.465466</td>\n",
              "      <td>0.859077</td>\n",
              "      <td>0.015660</td>\n",
              "      <td>vriksh</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>766</th>\n",
              "      <td>0.469656</td>\n",
              "      <td>0.332489</td>\n",
              "      <td>-0.155165</td>\n",
              "      <td>0.473491</td>\n",
              "      <td>0.319844</td>\n",
              "      <td>-0.137579</td>\n",
              "      <td>0.476636</td>\n",
              "      <td>0.319893</td>\n",
              "      <td>-0.137735</td>\n",
              "      <td>0.479524</td>\n",
              "      <td>...</td>\n",
              "      <td>0.476204</td>\n",
              "      <td>0.829023</td>\n",
              "      <td>0.107831</td>\n",
              "      <td>0.472267</td>\n",
              "      <td>0.711030</td>\n",
              "      <td>0.044306</td>\n",
              "      <td>0.465084</td>\n",
              "      <td>0.859327</td>\n",
              "      <td>0.029730</td>\n",
              "      <td>vriksh</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>767</th>\n",
              "      <td>0.470041</td>\n",
              "      <td>0.331704</td>\n",
              "      <td>-0.145384</td>\n",
              "      <td>0.473747</td>\n",
              "      <td>0.319283</td>\n",
              "      <td>-0.127014</td>\n",
              "      <td>0.476717</td>\n",
              "      <td>0.319353</td>\n",
              "      <td>-0.127165</td>\n",
              "      <td>0.479569</td>\n",
              "      <td>...</td>\n",
              "      <td>0.476365</td>\n",
              "      <td>0.828813</td>\n",
              "      <td>0.106276</td>\n",
              "      <td>0.471778</td>\n",
              "      <td>0.711524</td>\n",
              "      <td>0.044861</td>\n",
              "      <td>0.464475</td>\n",
              "      <td>0.860323</td>\n",
              "      <td>0.030196</td>\n",
              "      <td>vriksh</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>256 rows × 100 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-799f8e53-eda2-40a9-af16-a900788bbd4a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-799f8e53-eda2-40a9-af16-a900788bbd4a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-799f8e53-eda2-40a9-af16-a900788bbd4a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tad =  df_val[512:512+256]\n",
        "tad= tad.reset_index( drop=True)\n",
        "# tad_feat = tad.drop('1.1',1)"
      ],
      "metadata": {
        "id": "smFI6vRS9u91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class YogaDataset(Dataset):\n",
        "  def __init__(self, df):\n",
        "    self.features = df.drop(['1.1'], axis=1)\n",
        "    self.target = df['1.1']\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.features)\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    features = self.features.loc[index]\n",
        "    target = self.target[index]\n",
        "    return torch.tensor(features.tolist()).float().to(device)"
      ],
      "metadata": {
        "id": "OJLGT0isAJft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tad = YogaDataset(tad)\n",
        "tad_dl = DataLoader(tad, batch_size=16,drop_last=False)"
      ],
      "metadata": {
        "id": "ThTIqorZ_CWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lL5BHBHHCA_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in tad_dl:\n",
        "  \n",
        "  pred=(model(batch))\n",
        "  _, is_correct = torch.max(pred.data,1)\n",
        "  print(is_correct)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9ZB_bPi_Rq0",
        "outputId": "6512a2a0-4d1f-408b-f00b-4a37baea9b40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5])\n",
            "tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5])\n",
            "tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5])\n",
            "tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5])\n",
            "tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5])\n",
            "tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5])\n",
            "tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5])\n",
            "tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5])\n",
            "tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5])\n",
            "tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5])\n",
            "tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5])\n",
            "tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5])\n",
            "tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5])\n",
            "tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5])\n",
            "tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5])\n",
            "tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "{'bhujan': 0, 'padmasan': 1, 'shav': 2, 'tadasana': 3, 'trik': 4, 'vriksh': 5}"
      ],
      "metadata": {
        "id": "m_NiIHcC-BW6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}