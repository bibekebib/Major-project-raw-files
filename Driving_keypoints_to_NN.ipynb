{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "noAOCXnjE72s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0600cef-9b45-46af-dd89-c1d4107027c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "8FFWNvYGFbzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/keypoints_final_256.csv')\n",
        "df_test = pd.read_csv('/content/drive/MyDrive/keypoints_final_test_256.csv')"
      ],
      "metadata": {
        "id": "dlG98oieFm4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop('Unnamed: 0',1)\n",
        "df_test = df_test.drop('Unnamed: 0',1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEI3i23WFtV6",
        "outputId": "eac787fb-0ee7-4c4f-aed7-06dc51ce1362"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-185-2408e182c2cf>:1: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  df = df.drop('Unnamed: 0',1)\n",
            "<ipython-input-185-2408e182c2cf>:2: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  df_test = df_test.drop('Unnamed: 0',1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "1SDKesvSFuEe",
        "outputId": "e3926a9f-8e44-4e0e-bb22-ffa47cf02be7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              0         1         2         3         4         5         6  \\\n",
              "0      0.561012  0.302157 -0.123826  0.564638  0.289460 -0.105465  0.567299   \n",
              "1      0.561800  0.300378 -0.112502  0.564178  0.286860 -0.094147  0.566694   \n",
              "2      0.561128  0.303233 -0.398081  0.564855  0.286085 -0.388905  0.567303   \n",
              "3      0.560880  0.309663 -0.300727  0.564582  0.291333 -0.289611  0.567254   \n",
              "4      0.561824  0.313649 -0.342597  0.565599  0.294846 -0.333475  0.568605   \n",
              "...         ...       ...       ...       ...       ...       ...       ...   \n",
              "13124  0.486648  0.286260 -0.089110  0.490879  0.273558 -0.067470  0.493832   \n",
              "13125  0.482443  0.300422 -0.362747  0.487832  0.286010 -0.350695  0.491121   \n",
              "13126  0.480868  0.294958 -0.321305  0.485442  0.280226 -0.312262  0.488200   \n",
              "13127  0.476346  0.293068 -0.347669  0.480525  0.276444 -0.338560  0.483666   \n",
              "13128  0.474513  0.301189 -0.396490  0.478878  0.286351 -0.384223  0.482274   \n",
              "\n",
              "              7         8         9  ...        90        91        92  \\\n",
              "0      0.289761 -0.105632  0.569772  ...  0.533098  0.883884  0.116445   \n",
              "1      0.287539 -0.094309  0.568935  ...  0.532270  0.883887  0.118631   \n",
              "2      0.285346 -0.389124  0.569531  ...  0.527861  0.878289  0.133518   \n",
              "3      0.290768 -0.289883  0.570002  ...  0.536183  0.912010  0.188811   \n",
              "4      0.294382 -0.333750  0.571083  ...  0.540910  0.887583  0.192771   \n",
              "...         ...       ...       ...  ...       ...       ...       ...   \n",
              "13124  0.272559 -0.067595  0.497064  ...  0.470475  0.872472  0.102894   \n",
              "13125  0.285521 -0.350950  0.493022  ...  0.474411  0.877378  0.126974   \n",
              "13126  0.280182 -0.312505  0.491096  ...  0.473303  0.876180  0.188003   \n",
              "13127  0.276016 -0.338818  0.485902  ...  0.473899  0.877159  0.200288   \n",
              "13128  0.286848 -0.384487  0.484189  ...  0.473569  0.875810  0.184950   \n",
              "\n",
              "             93        94        95        96        97        98       1.1  \n",
              "0      0.566635  0.933335  0.036987  0.536138  0.938561 -0.000178  tadasana  \n",
              "1      0.570173  0.931963  0.035905  0.532538  0.933740 -0.001898  tadasana  \n",
              "2      0.566152  0.918341  0.058699  0.534654  0.928207 -0.001211  tadasana  \n",
              "3      0.567818  0.914286  0.152388  0.530323  0.931029  0.077729  tadasana  \n",
              "4      0.571397  0.923574  0.116451  0.531012  0.930395  0.070042  tadasana  \n",
              "...         ...       ...       ...       ...       ...       ...       ...  \n",
              "13124  0.496999  0.912968  0.022522  0.466042  0.915865 -0.025367  tadasana  \n",
              "13125  0.494784  0.912487  0.072589  0.465412  0.914555  0.006590  tadasana  \n",
              "13126  0.495985  0.914854  0.099370  0.465068  0.916696  0.075990  tadasana  \n",
              "13127  0.494451  0.915021  0.085202  0.464427  0.913910  0.084059  tadasana  \n",
              "13128  0.496280  0.913268  0.085682  0.464682  0.914772  0.071310  tadasana  \n",
              "\n",
              "[13129 rows x 100 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0d194f4e-d898-4e07-a7f2-67ebc169e49d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>1.1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.561012</td>\n",
              "      <td>0.302157</td>\n",
              "      <td>-0.123826</td>\n",
              "      <td>0.564638</td>\n",
              "      <td>0.289460</td>\n",
              "      <td>-0.105465</td>\n",
              "      <td>0.567299</td>\n",
              "      <td>0.289761</td>\n",
              "      <td>-0.105632</td>\n",
              "      <td>0.569772</td>\n",
              "      <td>...</td>\n",
              "      <td>0.533098</td>\n",
              "      <td>0.883884</td>\n",
              "      <td>0.116445</td>\n",
              "      <td>0.566635</td>\n",
              "      <td>0.933335</td>\n",
              "      <td>0.036987</td>\n",
              "      <td>0.536138</td>\n",
              "      <td>0.938561</td>\n",
              "      <td>-0.000178</td>\n",
              "      <td>tadasana</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.561800</td>\n",
              "      <td>0.300378</td>\n",
              "      <td>-0.112502</td>\n",
              "      <td>0.564178</td>\n",
              "      <td>0.286860</td>\n",
              "      <td>-0.094147</td>\n",
              "      <td>0.566694</td>\n",
              "      <td>0.287539</td>\n",
              "      <td>-0.094309</td>\n",
              "      <td>0.568935</td>\n",
              "      <td>...</td>\n",
              "      <td>0.532270</td>\n",
              "      <td>0.883887</td>\n",
              "      <td>0.118631</td>\n",
              "      <td>0.570173</td>\n",
              "      <td>0.931963</td>\n",
              "      <td>0.035905</td>\n",
              "      <td>0.532538</td>\n",
              "      <td>0.933740</td>\n",
              "      <td>-0.001898</td>\n",
              "      <td>tadasana</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.561128</td>\n",
              "      <td>0.303233</td>\n",
              "      <td>-0.398081</td>\n",
              "      <td>0.564855</td>\n",
              "      <td>0.286085</td>\n",
              "      <td>-0.388905</td>\n",
              "      <td>0.567303</td>\n",
              "      <td>0.285346</td>\n",
              "      <td>-0.389124</td>\n",
              "      <td>0.569531</td>\n",
              "      <td>...</td>\n",
              "      <td>0.527861</td>\n",
              "      <td>0.878289</td>\n",
              "      <td>0.133518</td>\n",
              "      <td>0.566152</td>\n",
              "      <td>0.918341</td>\n",
              "      <td>0.058699</td>\n",
              "      <td>0.534654</td>\n",
              "      <td>0.928207</td>\n",
              "      <td>-0.001211</td>\n",
              "      <td>tadasana</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.560880</td>\n",
              "      <td>0.309663</td>\n",
              "      <td>-0.300727</td>\n",
              "      <td>0.564582</td>\n",
              "      <td>0.291333</td>\n",
              "      <td>-0.289611</td>\n",
              "      <td>0.567254</td>\n",
              "      <td>0.290768</td>\n",
              "      <td>-0.289883</td>\n",
              "      <td>0.570002</td>\n",
              "      <td>...</td>\n",
              "      <td>0.536183</td>\n",
              "      <td>0.912010</td>\n",
              "      <td>0.188811</td>\n",
              "      <td>0.567818</td>\n",
              "      <td>0.914286</td>\n",
              "      <td>0.152388</td>\n",
              "      <td>0.530323</td>\n",
              "      <td>0.931029</td>\n",
              "      <td>0.077729</td>\n",
              "      <td>tadasana</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.561824</td>\n",
              "      <td>0.313649</td>\n",
              "      <td>-0.342597</td>\n",
              "      <td>0.565599</td>\n",
              "      <td>0.294846</td>\n",
              "      <td>-0.333475</td>\n",
              "      <td>0.568605</td>\n",
              "      <td>0.294382</td>\n",
              "      <td>-0.333750</td>\n",
              "      <td>0.571083</td>\n",
              "      <td>...</td>\n",
              "      <td>0.540910</td>\n",
              "      <td>0.887583</td>\n",
              "      <td>0.192771</td>\n",
              "      <td>0.571397</td>\n",
              "      <td>0.923574</td>\n",
              "      <td>0.116451</td>\n",
              "      <td>0.531012</td>\n",
              "      <td>0.930395</td>\n",
              "      <td>0.070042</td>\n",
              "      <td>tadasana</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13124</th>\n",
              "      <td>0.486648</td>\n",
              "      <td>0.286260</td>\n",
              "      <td>-0.089110</td>\n",
              "      <td>0.490879</td>\n",
              "      <td>0.273558</td>\n",
              "      <td>-0.067470</td>\n",
              "      <td>0.493832</td>\n",
              "      <td>0.272559</td>\n",
              "      <td>-0.067595</td>\n",
              "      <td>0.497064</td>\n",
              "      <td>...</td>\n",
              "      <td>0.470475</td>\n",
              "      <td>0.872472</td>\n",
              "      <td>0.102894</td>\n",
              "      <td>0.496999</td>\n",
              "      <td>0.912968</td>\n",
              "      <td>0.022522</td>\n",
              "      <td>0.466042</td>\n",
              "      <td>0.915865</td>\n",
              "      <td>-0.025367</td>\n",
              "      <td>tadasana</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13125</th>\n",
              "      <td>0.482443</td>\n",
              "      <td>0.300422</td>\n",
              "      <td>-0.362747</td>\n",
              "      <td>0.487832</td>\n",
              "      <td>0.286010</td>\n",
              "      <td>-0.350695</td>\n",
              "      <td>0.491121</td>\n",
              "      <td>0.285521</td>\n",
              "      <td>-0.350950</td>\n",
              "      <td>0.493022</td>\n",
              "      <td>...</td>\n",
              "      <td>0.474411</td>\n",
              "      <td>0.877378</td>\n",
              "      <td>0.126974</td>\n",
              "      <td>0.494784</td>\n",
              "      <td>0.912487</td>\n",
              "      <td>0.072589</td>\n",
              "      <td>0.465412</td>\n",
              "      <td>0.914555</td>\n",
              "      <td>0.006590</td>\n",
              "      <td>tadasana</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13126</th>\n",
              "      <td>0.480868</td>\n",
              "      <td>0.294958</td>\n",
              "      <td>-0.321305</td>\n",
              "      <td>0.485442</td>\n",
              "      <td>0.280226</td>\n",
              "      <td>-0.312262</td>\n",
              "      <td>0.488200</td>\n",
              "      <td>0.280182</td>\n",
              "      <td>-0.312505</td>\n",
              "      <td>0.491096</td>\n",
              "      <td>...</td>\n",
              "      <td>0.473303</td>\n",
              "      <td>0.876180</td>\n",
              "      <td>0.188003</td>\n",
              "      <td>0.495985</td>\n",
              "      <td>0.914854</td>\n",
              "      <td>0.099370</td>\n",
              "      <td>0.465068</td>\n",
              "      <td>0.916696</td>\n",
              "      <td>0.075990</td>\n",
              "      <td>tadasana</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13127</th>\n",
              "      <td>0.476346</td>\n",
              "      <td>0.293068</td>\n",
              "      <td>-0.347669</td>\n",
              "      <td>0.480525</td>\n",
              "      <td>0.276444</td>\n",
              "      <td>-0.338560</td>\n",
              "      <td>0.483666</td>\n",
              "      <td>0.276016</td>\n",
              "      <td>-0.338818</td>\n",
              "      <td>0.485902</td>\n",
              "      <td>...</td>\n",
              "      <td>0.473899</td>\n",
              "      <td>0.877159</td>\n",
              "      <td>0.200288</td>\n",
              "      <td>0.494451</td>\n",
              "      <td>0.915021</td>\n",
              "      <td>0.085202</td>\n",
              "      <td>0.464427</td>\n",
              "      <td>0.913910</td>\n",
              "      <td>0.084059</td>\n",
              "      <td>tadasana</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13128</th>\n",
              "      <td>0.474513</td>\n",
              "      <td>0.301189</td>\n",
              "      <td>-0.396490</td>\n",
              "      <td>0.478878</td>\n",
              "      <td>0.286351</td>\n",
              "      <td>-0.384223</td>\n",
              "      <td>0.482274</td>\n",
              "      <td>0.286848</td>\n",
              "      <td>-0.384487</td>\n",
              "      <td>0.484189</td>\n",
              "      <td>...</td>\n",
              "      <td>0.473569</td>\n",
              "      <td>0.875810</td>\n",
              "      <td>0.184950</td>\n",
              "      <td>0.496280</td>\n",
              "      <td>0.913268</td>\n",
              "      <td>0.085682</td>\n",
              "      <td>0.464682</td>\n",
              "      <td>0.914772</td>\n",
              "      <td>0.071310</td>\n",
              "      <td>tadasana</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>13129 rows × 100 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0d194f4e-d898-4e07-a7f2-67ebc169e49d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0d194f4e-d898-4e07-a7f2-67ebc169e49d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0d194f4e-d898-4e07-a7f2-67ebc169e49d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 186
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "TsIIleLCF0kQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lbe = LabelEncoder()\n",
        "df['1.1']= lbe.fit_transform(df['1.1'])\n",
        "df_test['1.1']= lbe.fit_transform(df_test['1.1'])\n",
        "dict(zip(lbe.classes_, lbe.transform(lbe.classes_)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mheSh8XMrXRy",
        "outputId": "425d3aed-795b-47bd-b311-beaf38ce5622"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bhujan': 0, 'padmasan': 1, 'shav': 2, 'tadasana': 3, 'trik': 4, 'vriksh': 5}"
            ]
          },
          "metadata": {},
          "execution_count": 188
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "P8HAO9N7sX4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class YogaDataset(Dataset):\n",
        "  def __init__(self, df):\n",
        "    self.features = df.drop(['1.1'], axis=1)\n",
        "    self.target = df['1.1']\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.features)\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    features = self.features.loc[index]\n",
        "    target = self.target[index]\n",
        "    return torch.tensor(features.tolist()).float().to(device), torch.tensor(target).long().to(device)"
      ],
      "metadata": {
        "id": "04UjZs3SGFt9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_feat = df.drop(['1.1'], axis=1)"
      ],
      "metadata": {
        "id": "kjn1diRCPPBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "oLFPQdOlPo4_",
        "outputId": "519eba3f-9cd6-499d-8305-4f07a5ba62b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 192
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# target = df['1.1']"
      ],
      "metadata": {
        "id": "6rS3CyegPVHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = YogaDataset(df)"
      ],
      "metadata": {
        "id": "AUf74C14Pc3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feat, target = data[100]"
      ],
      "metadata": {
        "id": "LIEICAjrPi7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZD7k54HhXok",
        "outputId": "5cffc936-cfa8-46fa-adc7-f17a8722d13a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3)"
            ]
          },
          "metadata": {},
          "execution_count": 195
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN(nn.Module):\n",
        "  def __init__(self,hidden_dim=20, input_dim=99, sequence_num=16, n_layers=1):\n",
        "    super(RNN, self).__init__()\n",
        "    self.input_dim = input_dim\n",
        "    self.sequence_num = sequence_num\n",
        "    self.n_layers = n_layers\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.rnn = nn.RNN(input_dim,hidden_dim, n_layers, batch_first=True)\n",
        "    self.fc = nn.Linear(hidden_dim, 6)\n",
        "  def forward(self, input):\n",
        "    h0 = torch.zeros(self.n_layers,  self.hidden_dim)\n",
        "    out, _ = self.rnn(input,h0)\n",
        "    pred = self.fc(out)\n",
        "    output = nn.Softmax( dim=1)(pred)\n",
        "    return output"
      ],
      "metadata": {
        "id": "NHqLquJOhgNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = RNN().to(device)"
      ],
      "metadata": {
        "id": "1RWWhgIds98B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "JH5Wcyq4tAd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "LUqs5IautI7r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f49b9b1-e3a2-4b54-90e5-369f95730f59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNN(\n",
              "  (rnn): RNN(99, 20, batch_first=True)\n",
              "  (fc): Linear(in_features=20, out_features=6, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 199
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xzg1h1i1g9KR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data():\n",
        "    train = YogaDataset(df)\n",
        "    test = YogaDataset(df_test)\n",
        "    trn_dl = DataLoader(train, batch_size=16,shuffle=True, drop_last=True)\n",
        "    test_dl = DataLoader(test, batch_size=16, shuffle=True, drop_last=True)\n",
        "    return trn_dl, test_dl"
      ],
      "metadata": {
        "id": "lslJszPoXFq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trn_ldr, test_ldr = get_data()\n",
        "# for x in trn_ldr:\n",
        "#   print(x)\n",
        "#   break"
      ],
      "metadata": {
        "id": "EMNUp4xCXnuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_batch(x, y, model, loss_fn, opt):\n",
        "    model.train()\n",
        "    prediction = model(x)\n",
        "    # print(y.shape)\n",
        "    # print(y)\n",
        "    # print(prediction.shape)\n",
        "    batch_loss = loss_fn(prediction, y)\n",
        "    batch_loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    return batch_loss.item()"
      ],
      "metadata": {
        "id": "Wf3uvwm2Xwjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def accuracy(x, y, model):\n",
        "    model.eval()\n",
        "    pred = model(x)\n",
        "    _, is_correct = torch.max(pred.data,1)\n",
        "    acc_all = (is_correct==y).detach().cpu().numpy()\n",
        "    return acc_all"
      ],
      "metadata": {
        "id": "PEY2jjXIX3nZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def val_loss_trn(x, y, model, loss_fn):\n",
        "  prediction = model(x)\n",
        "  val_loss = loss_fn(prediction, y)\n",
        "  return val_loss.item()"
      ],
      "metadata": {
        "id": "k4TGtcVqjmIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss = []\n",
        "train_accuracies= []\n",
        "val_loss = [] \n",
        "val_accuracies = []\n",
        "\n",
        "for i in range(10):\n",
        "    print(f'Epoch: _________*****{i}*****_______')\n",
        "    train_epoch_losses, train_epoch_accuracies = [], []\n",
        "    val_epoch_accuracies, val_epoch_losses = [], []\n",
        "\n",
        "\n",
        "\n",
        "    for ix, batch in (enumerate(iter(trn_ldr))):\n",
        "\n",
        "        x, y = batch\n",
        "        train_epoch_losses.append(train_batch(x, y, model, loss_fn, optimizer))\n",
        "    train_epoch_loss = np.array(train_epoch_losses).mean()\n",
        "    print(f'Epoch: _________*****{i} Training Loss : {train_epoch_loss} *****_______')\n",
        "\n",
        "\n",
        "\n",
        "    for ix, batch in (enumerate(iter(trn_ldr))):\n",
        "        x, y = batch\n",
        "        # x = x.permute(0, 3,1,2)\n",
        "        train_epoch_accuracies.append(sum(accuracy(x, y, model)) / len(y))\n",
        "    train_epoch_accuracy = np.array(train_epoch_accuracies).mean()\n",
        "    print(f'Epoch: _________*****{i} Training Accuracy: {train_epoch_accuracy} *****_______')\n",
        "\n",
        "\n",
        "    for ix, batch in (enumerate(iter(test_ldr))):\n",
        "        x, y = batch\n",
        "        # x = x.permute(0, 3,1,2)\n",
        "        val_epoch_losses.append(val_loss_trn(x, y, model, loss_fn))\n",
        "    val_epoch_loss = np.array(val_epoch_losses).mean()\n",
        "    print(f'Epoch: _________*****{i} Validation Loss : {val_epoch_loss} *****_______')\n",
        "\n",
        "    for ix, batch in (enumerate(iter(test_ldr))):\n",
        "        x, y = batch\n",
        "        # x = x.permute(0, 3,1,2)\n",
        "        val_epoch_accuracies.append(sum(accuracy(x, y, model)) / len(y))\n",
        "    val_epoch_accuracy = np.array(val_epoch_accuracies).mean()\n",
        "    print(f'Epoch: _________*****{i} Validation Accuracy. {val_epoch_accuracy} *****_______')\n",
        "    print('\\n')\n",
        "\n",
        "    train_loss.append(train_epoch_loss)\n",
        "    train_accuracies.append(train_epoch_accuracy)\n",
        "    val_accuracies.append(val_epoch_accuracy)\n",
        "    val_loss.append(val_epoch_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLoo8XM3X6Pn",
        "outputId": "6c773c37-98a7-4a7c-a169-3715d499967a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: _________*****0*****_______\n",
            "Epoch: _________*****0 Training Loss : 1.3447024740823885 *****_______\n",
            "Epoch: _________*****0 Training Accuracy: 0.9511432926829269 *****_______\n",
            "Epoch: _________*****0 Validation Loss : 1.116986921440018 *****_______\n",
            "Epoch: _________*****0 Validation Accuracy. 0.9653745644599303 *****_______\n",
            "\n",
            "\n",
            "Epoch: _________*****1*****_______\n",
            "Epoch: _________*****1 Training Loss : 1.1100339143741422 *****_______\n",
            "Epoch: _________*****1 Training Accuracy: 0.965625 *****_______\n",
            "Epoch: _________*****1 Validation Loss : 1.095366268623166 *****_______\n",
            "Epoch: _________*****1 Validation Accuracy. 0.9577526132404182 *****_______\n",
            "\n",
            "\n",
            "Epoch: _________*****2*****_______\n",
            "Epoch: _________*****2 Training Loss : 1.0865559181062188 *****_______\n",
            "Epoch: _________*****2 Training Accuracy: 0.9766768292682927 *****_______\n",
            "Epoch: _________*****2 Validation Loss : 1.089306939769705 *****_______\n",
            "Epoch: _________*****2 Validation Accuracy. 0.9570993031358885 *****_______\n",
            "\n",
            "\n",
            "Epoch: _________*****3*****_______\n",
            "Epoch: _________*****3 Training Loss : 1.0756648246834917 *****_______\n",
            "Epoch: _________*****3 Training Accuracy: 0.9798780487804878 *****_______\n",
            "Epoch: _________*****3 Validation Loss : 1.0666019347486595 *****_______\n",
            "Epoch: _________*****3 Validation Accuracy. 0.9849738675958188 *****_______\n",
            "\n",
            "\n",
            "Epoch: _________*****4*****_______\n",
            "Epoch: _________*****4 Training Loss : 1.0701330510581413 *****_______\n",
            "Epoch: _________*****4 Training Accuracy: 0.9809451219512195 *****_______\n",
            "Epoch: _________*****4 Validation Loss : 1.072774183044035 *****_______\n",
            "Epoch: _________*****4 Validation Accuracy. 0.9769163763066202 *****_______\n",
            "\n",
            "\n",
            "Epoch: _________*****5*****_______\n",
            "Epoch: _________*****5 Training Loss : 1.0667041534330788 *****_______\n",
            "Epoch: _________*****5 Training Accuracy: 0.9836128048780488 *****_______\n",
            "Epoch: _________*****5 Validation Loss : 1.0622070583316923 *****_______\n",
            "Epoch: _________*****5 Validation Accuracy. 0.9871515679442509 *****_______\n",
            "\n",
            "\n",
            "Epoch: _________*****6*****_______\n",
            "Epoch: _________*****6 Training Loss : 1.0640887702383646 *****_______\n",
            "Epoch: _________*****6 Training Accuracy: 0.9833841463414634 *****_______\n",
            "Epoch: _________*****6 Validation Loss : 1.0646244461943464 *****_______\n",
            "Epoch: _________*****6 Validation Accuracy. 0.9849738675958188 *****_______\n",
            "\n",
            "\n",
            "Epoch: _________*****7*****_______\n",
            "Epoch: _________*****7 Training Loss : 1.0626224120942558 *****_______\n",
            "Epoch: _________*****7 Training Accuracy: 0.9857469512195122 *****_______\n",
            "Epoch: _________*****7 Validation Loss : 1.0576632097623073 *****_______\n",
            "Epoch: _________*****7 Validation Accuracy. 0.9871515679442509 *****_______\n",
            "\n",
            "\n",
            "Epoch: _________*****8*****_______\n",
            "Epoch: _________*****8 Training Loss : 1.0610466263643126 *****_______\n",
            "Epoch: _________*****8 Training Accuracy: 0.9861280487804878 *****_______\n",
            "Epoch: _________*****8 Validation Loss : 1.0589095067479468 *****_______\n",
            "Epoch: _________*****8 Validation Accuracy. 0.9869337979094077 *****_______\n",
            "\n",
            "\n",
            "Epoch: _________*****9*****_______\n",
            "Epoch: _________*****9 Training Loss : 1.0602010258814183 *****_______\n",
            "Epoch: _________*****9 Training Accuracy: 0.9863567073170731 *****_______\n",
            "Epoch: _________*****9 Validation Loss : 1.0567642130502841 *****_______\n",
            "Epoch: _________*****9 Validation Accuracy. 0.9875871080139372 *****_______\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fE2v2W1UYH2_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}